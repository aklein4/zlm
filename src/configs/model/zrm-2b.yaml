defaults:
  - _self_  # refers to this config file
  - sharding: zrm-fsdp  # refers to sharding/llama-fsdp.yaml
  - remat: zrm  # refers to remat/llama-scan.yaml


model_id: zrm-2b
model_class: zrm.ZRMModel  # Used to import the model from this class
pretrained_model: null
pretrained_step: null

vocab_size: 32101
bos_token_id: 1
eos_token_id: 2
pad_token_id: 32000

hidden_size: 2048
num_hidden_layers: 22

num_attention_heads: 16
num_key_value_heads: 8

intermediate_size: 5120
hidden_act: silu

max_position_embeddings: 2048
rope_theta: 10000.0

attention_dropout: false
attention_bias: false

initializer_range: 0.02
rms_norm_eps: 1.0e-05

# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: flash_attention

input_length: 512
output_length: 512
z_length: 768

z_size: 128

pad_bias: -20.0
