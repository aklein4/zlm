
# LM
embed_tokens.weight: [fsdp, null]
lm_head.weight: [fsdp, null]


# encoder
encoder.layers.*.input_layernorm.weight: [null, fsdp]
encoder.layers.*.input_layernorm.bias: [null, fsdp]

encoder.layers.*.self_attn.qkv_proj.weight: [fsdp, null]
encoder.layers.*.self_attn.o_proj.base_linear.weight: [fsdp, null]
encoder.layers.*.self_attn.o_proj.out_weight: [null, fsdp]

encoder.layers.*.bi_attn_norm.weight: [null, fsdp]
encoder.layers.*.bi_attn_norm.bias: [null, fsdp]

encoder.layers.*.bi_attn.qkv_proj.weight: [fsdp, null]
encoder.layers.*.bi_attn.o_proj.base_linear.weight: [fsdp, null]
encoder.layers.*.bi_attn.o_proj.out_weight: [null, fsdp]

encoder.layers.*.post_attention_layernorm.weight: [null, fsdp]
encoder.layers.*.post_attention_layernorm.bias: [null, fsdp]

encoder.layers.*.mlp.gate_up_proj.weight: [fsdp, null]
encoder.layers.*.mlp.down_proj.base_linear.weight: [null, fsdp]
encoder.layers.*.mlp.down_proj.out_weight: [null, fsdp]

encoder.norm.weight: [fsdp]


# generator
generator.layers.*.input_layernorm.weight: [null, fsdp]
generator.layers.*.input_layernorm.bias: [null, fsdp]

generator.layers.*.self_attn.qkv_proj.weight: [fsdp, null]
generator.layers.*.self_attn.o_proj.base_linear.weight: [fsdp, null]
generator.layers.*.self_attn.o_proj.out_weight: [null, fsdp]

generator.layers.*.post_attention_layernorm.weight: [null, fsdp]
generator.layers.*.post_attention_layernorm.bias: [null, fsdp]

generator.layers.*.mlp.gate_up_proj.weight: [fsdp, null]
generator.layers.*.mlp.down_proj.base_linear.weight: [null, fsdp]
generator.layers.*.mlp.down_proj.out_weight: [null, fsdp]

generator.norm.weight: [fsdp]


# decoder
decoder.layers.*.input_layernorm.weight: [null, fsdp]
decoder.layers.*.input_layernorm.bias: [null, fsdp]

decoder.layers.*.self_attn.qkv_proj.weight: [fsdp, null]
decoder.layers.*.self_attn.o_proj.base_linear.weight: [fsdp, null]
decoder.layers.*.self_attn.o_proj.out_weight: [null, fsdp]

decoder.layers.*.post_attention_layernorm.weight: [null, fsdp]
decoder.layers.*.post_attention_layernorm.bias: [null, fsdp]

decoder.layers.*.mlp.gate_up_proj.weight: [fsdp, null]
decoder.layers.*.mlp.down_proj.base_linear.weight: [null, fsdp]
decoder.layers.*.mlp.down_proj.out_weight: [null, fsdp]

decoder.norm.weight: [fsdp]


# inputs
encoder_input_emb: [null, fsdp]
encoder_sep_emb: [fsdp]
encoder_output_emb: [null, fsdp]
encoder_z_tokens: [null, fsdp]

generator_input_emb: [null, fsdp]
generator_z_tokens: [null, fsdp]

decoder_input_emb: [null, fsdp]
decoder_z_tokens: [null, fsdp]
decoder_start_output_token: [fsdp]
decoder_output_emb: [null, fsdp]


# projections
encoder_noise_proj_in.weight: [fsdp, null]
encoder_mu_proj_out.weight: [null, fsdp]

generator_z_proj_in.weight: [fsdp, null]
generator_mu_proj_out.weight: [null, fsdp]

decoder_z_proj_in.weight: [fsdp, null]


# extras


# Activations
# decoder.layers.*: [[data, fsdp], null, null]
# generator.layers.*: [[data, fsdp], null, null]
# encoder.layers.*: [[data, fsdp], null, null]

# lm_head: [[data, fsdp], null, null]

# encoder_noise_proj_in: [[data, fsdp], null, null]
# encoder_mu_proj_out: [[data, fsdp], null, null]

# generator_z_proj_in: [[data, fsdp], null, null]
# generator_mu_proj_out: [[data, fsdp], null, null]

# decoder_z_proj_in: [[data, fsdp], null, null]
