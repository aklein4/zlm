defaults:
  - _self_  # refers to this config file
  - sharding: zlm-fsdp  # refers to sharding/llama-fsdp.yaml
  - remat: zlm  # refers to remat/llama-scan.yaml


# Used to import the model from this class
type: zlm.ZLMModel

# Information about loading from a checkpoint
pretrained_url: null
pretrained_step: null
pretrained_strict: null

# torch information
torch_dtype: float32

# vocab information
vocab_size: 49152
bos_token_id: 0
eos_token_id: 0
pad_token_id: 49152

# architecture size
hidden_size: 2048
num_hidden_layers: 24

# attention configuration
num_attention_heads: 32
num_key_value_heads: 32

# MLP configuration
intermediate_size: 8192
hidden_act: silu

# rope configuration
max_position_embeddings: 8192
rope_theta: 130000

# misc
attention_dropout: false
attention_bias: false
initializer_range: 0.02 # should be unused bc of gaussian init
rms_norm_eps: 1.0e-05

# value added to pad token attention scores
# only used when elementwise_pad_mask is passed
pad_attention_bias_value: -100.0

# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: flash_attention

# pretrained model to use as base
pretrained_llama: aklein4/SmolLM2-1.7B-TPU

# lengths
input_length: 256
output_length: 512
z_length: 384

# latent configuration
latent_size: 32

# batch norm configuration
batch_norm_beta: 0.95
once_norm: false

# diffusion configuration
minimum_diffusion_timestep: 0.5
num_diffusion_timesteps: 16

# diffusion head configuration
diffusion_in_proj: false
num_diffusion_head_layers: 2
diffusion_mlp_size: 3072

# init configuration
init_sep_token_id: 198
init_z_token_message: [16339, 1103, 253, 1732, 876, 1129, 3073, 827, 15852, 6223, 42, 3039, 1833, 411, 1833, 284, 33748, 732, 346, 699, 588, 1869, 30, 5970, 29, 1717, 29, 5528, 3039, 314, 260, 2327, 338, 7488, 346, 3703, 43, 33748, 314, 260, 38354, 338, 6200, 346, 1991, 346, 359, 1361, 335, 260, 1048, 3398, 30, 16269, 1592, 28, 502, 724, 253, 1055, 2951, 6554, 28, 6063, 9101, 1584, 28, 284, 919, 11621, 2894, 908, 645, 260, 1732, 7422, 6991, 30, 198, 198, 3512, 29, 1717, 29, 5528, 3039, 5909, 411, 6939, 253, 21765, 3775, 618, 253, 2437, 2370, 30, 378, 1055, 2496, 732, 260, 1732, 314, 5617, 28, 14316, 260, 619, 28728, 573, 1096, 28, 284, 3834, 260, 6552, 99, 30, 1069, 654, 34013, 260, 1962, 281, 480, 1038, 1924, 28, 975, 29532, 260, 3491, 314, 260, 17938, 970, 288, 3368, 3867, 30, 9528, 28, 502, 3525, 253, 1252, 30, 1848, 1252, 1124, 325, 253, 1343, 1341, 365, 2579, 8659, 253, 3856, 618, 3764, 2391, 28, 5510, 253, 11262, 28, 15656, 10195, 28, 355, 3728, 3480, 643, 355, 357, 1124, 325, 253, 2232, 3062, 715, 347, 619, 7298, 339, 417, 764, 4960, 5307, 28, 965, 339, 417, 764, 6811, 3416, 1184, 1814, 971, 1833, 28, 502, 1188, 582, 1165, 2353, 284, 2545, 732, 357, 7124, 30, 1094, 260, 2353, 1072, 441, 2369, 10443, 28, 502, 4211, 42, 1576, 253, 896, 2424, 28, 1492, 327, 6782, 1096, 28, 355, 1532, 368, 260, 1732, 1163, 30, 198, 198, 1348, 314, 837, 33748, 3207, 1895, 30, 2550, 11915, 253, 6579, 20430, 282, 746, 1265, 16593, 253, 3246, 28, 17085, 253, 1685, 28, 3728, 253, 1671, 28, 355, 7782, 253, 9704, 1265, 1195, 1055, 43216, 284, 30557, 732, 502, 457, 2269, 588, 1869, 30, 330, 1123, 7127, 314, 1890, 28, 7257, 28, 284, 1028, 515, 30, 657, 1124, 2139, 702, 42, 619, 2931, 1869, 28, 260, 1646, 24653, 314, 2273, 1447, 355, 619, 504, 940, 4567, 330, 314, 2474, 645, 389, 12191, 1447, 355, 619, 57, 417, 307, 15129, 827, 3416, 975, 502, 26599, 260, 4292, 1184, 6567, 93, 2372, 1746, 260, 1055, 429, 7980, 1130, 29025, 22912, 30, 1069, 1607, 13490, 6826, 618, 253, 1165, 932, 282, 5307, 338, 416, 3948, 260, 1867, 1833, 30, 198, 198, 2696, 392, 359, 4421, 288, 1956, 2385, 653, 3403, 3564, 1535, 30]

z_proj_in_init_scale: 0.5
