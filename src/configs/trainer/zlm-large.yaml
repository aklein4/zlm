
# vvvvvvvvvv Don't forget to change this for new experiments vvvvvvvvvv
type: zlm_trainer.ZLMTrainer  # Used to import the trainer from this class


# total batch size across all devices
global_batch_size: 512

# run for N steps
max_steps: 1000000

# Save a checkpoint every N steps
checkpoint_interval: 1000 


# optimizer configuration
optimizer:

  type: adamw.AdamW

  kwargs:

    lr: 1.0e-4

    betas: [0.9, 0.98]
    
    eps: 1e-5

    weight_decay: 0.1

    update_clip: null


# Learning rate scheduler configuration
lr_scheduler:

  type: warmup_stable_decay

  # get_scheduler requires this outside of kwargs
  num_warmup_steps: 1000

  kwargs:

    num_stable_steps: 1000000
    num_decay_steps: 5000

    min_lr_ratio: 0.0

    warmup_type: linear
    decay_type: linear


# Defaults to clip the L2 norm of gradients to 1.0.
# Set to null to disable gradient clipping by norm.
max_grad_norm: 1.0
# Defaults to not clip gradients by their absolute value.
# Set to a number to clip gradients by the specified max absolute value.
max_grad_value: null

# parameter configuration
freeze_decoder: false

# hook configuration
init_hook: false
init_hook_step: null
hook_warmup_steps: 1000
hook_wait_steps: 0

# threshold configuration
upper_loss_threshold: 0.75
lower_loss_threshold: 0.5
min_lm_loss_scale: 0.05

# diffusion sampling configuration
num_diffusion_samples: 4
num_uncond_diffusion_samples: 1

# loss weighting configuration
kl_weight_relu_shift: 0.0
beta: 0.2
