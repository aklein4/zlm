
# vvvvvvvvvv Don't forget to change this for new experiments vvvvvvvvvv
type: seq_to_seq_lm_trainer.SeqToSeqLMTrainer  # Used to import the trainer from this class


# total batch size across all devices
global_batch_size: 512

# run for N steps
max_steps: 1000000 

# Save a checkpoint every N steps
checkpoint_interval: 1000000 


# optimizer configuration
optimizer:

  type: adamw.AdamW

  kwargs:

    lr: 3.0e-4

    betas: [0.9, 0.95]
    
    eps: 1e-6

    weight_decay: 0.1

    update_clip: null


# Learning rate scheduler configuration
lr_scheduler:

  type: linear

  warmup_steps: 500
  training_steps: 1000000
  

# Defaults to clip the L2 norm of gradients to 1.0.
# Set to null to disable gradient clipping by norm.
max_grad_norm: 1.0
# Defaults to not clip gradients by their absolute value.
# Set to a number to clip gradients by the specified max absolute value.
max_grad_value: null
