
# vvvvvvvvvv Don't forget to change this for new experiments vvvvvvvvvv
type: zlm_trainer.ZLMTrainer  # Used to import the trainer from this class


# total batch size across all devices
global_batch_size: 512

# run for N steps
max_steps: 1000000

# Save a checkpoint every N steps
checkpoint_interval: 2500 


# optimizer configuration
optimizer:

  type: adamw.AdamW

  kwargs:

    lr: 3.0e-4

    betas: [0.9, 0.95]
    
    eps: 1e-6

    weight_decay: 0.1

    update_clip: null


# Learning rate scheduler configuration
lr_scheduler:

  type: linear

  warmup_steps: 100
  training_steps: 1000000
  

# Defaults to clip the L2 norm of gradients to 1.0.
# Set to null to disable gradient clipping by norm.
max_grad_norm: 1.0
# Defaults to not clip gradients by their absolute value.
# Set to a number to clip gradients by the specified max absolute value.
max_grad_value: null

# hook configuration
init_hook: false
init_hook_step: null
hook_warmup_steps: 1000
hook_wait_steps: 2000

# threshold configuration
upper_loss_threshold: 0.5
lower_loss_threshold: 0.25
min_lm_loss_scale: 0.01

# diffusion sampling configuration
num_diffusion_samples: 4
num_uncond_diffusion_samples: 1

# loss weighting configuration
kl_weight_relu_shift: 0.5
beta: 0.1
