# The default config file. You may override configs with `key=value` arguments on the CLI
# according to https://hydra.cc/docs/advanced/override_grammar/basic/.

# This defines the order in which configs are loaded. The latter configs
# override the earlier ones.
defaults:
  - _self_ # refers to this config file
  - model: smollm2-test 
  - data: mixed-smollm2
  - trainer: lm-test


# compute configuration
seed: 42

torch_dtype: float32 # must be float32 for now?


# logging and saving configuration
debug: false

project: tpu-testing
name: test

notes: null


# The virtual device mesh shape to use within a TPU slice. This is also called
# the "ICI mesh", since devices within a slice enjoy a faster network called
# "Inter-Chip Interconnect".
ici_mesh:
  data: 1
  fsdp: 16
  tensor: 1
  expert: 1
  context: 1


# Shape of the logical mesh where each element is a TPU slice. This is called
# "Data Center Network (DCN) mesh" because TPU slices are usually connected
# together with slower data center networking, with the faster ICI network
# used within a slice.
#
# As an example, to enable 2-way data parallelism across 2 TPU slices, you may
# specify `dcn_mesh.data=2`.
dcn_mesh:
  data: 1
  fsdp: 1
  tensor: 1
  expert: 1
  context: 1


# These are default values for model activation rematerialization configuration.
# They can be overridden on the command line or by importing one of the presets
# in the `model/remat` directory.
model:

  # Name of classes in the module tree that are functionally pure.
  #
  # There are a few advantages of wrapping a module whose forward pass you know is
  # free of side-effects and whose behavior only depends on inputs in a `PureModule`:
  # - `PureModule`s will only be traced once.
  # - Framework profile scopes added via `xp.Trace` will show up in both the forward
  #   and the backward pass.
  pure_modules: []

  # Options for controlling tensor rematerialization.
  remat:
  
    # The class names of model layers whose intermediate activations should be
    # recomputed during the backward pass (i.e. activation checkpointing).
    activation_checkpoint_layers: []

    # If not null, compile a module of type `HomogeneousSequential` located at the
    # given path in the module tree using `torch_xla.experimental.scan_layers`.
    scan_layers: null

    # If specified, offload these tensors to host RAM during the forward pass and
    # move them back during the backward pass.
    #
    # The tensors to be offloaded should be given a name by wrapping them with the
    # `torchprime.torch_xla_models.offloading.offload_name` call. Then the same
    # name could be specified here to offload that tensor.
    #
    # Currently in order to offload tensors, `scan_layers` must also be enabled.
    offload_tensors: []
