
trainer_class: llm_trainer.LLMTrainer  # Used to import the trainer from this class

# This is for basic training loop, i.e., forward/backward pass without any special task logic.
global_batch_size: 256
max_steps: 160000

# checkpointing configuration
checkpoint_interval: 5000  # Save a checkpoint every N steps
# convert_to_safetensors: true  # Convert the model to safetensors format when saving

# Optimizer configuration.
optimizer:
  type: adamw

  learning_rate: 1.0e-4
  weight_decay: 0.1

  beta1: 0.9
  beta2: 0.95

  update_clip: 3.0


# Learning rate scheduler configuration.
lr_scheduler:
  type: linear

  warmup_steps: 2000
  training_steps: 1000000
  

# Defaults to clip the L2 norm of gradients to 1.0.
# Set to null to disable gradient clipping by norm.
max_grad_norm: 1.0
# Defaults to not clip gradients by their absolute value.
# Set to a number to clip gradients by the specified max absolute value.
max_grad_value: null
