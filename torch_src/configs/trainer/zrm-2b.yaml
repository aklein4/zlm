
trainer_class: zrm_trainer.ZRMTrainer  # Used to import the trainer from this class

# This is for basic training loop, i.e., forward/backward pass without any special task logic.
global_batch_size: 256
max_steps: 160000

# checkpointing configuration
checkpoint_interval: 5000  # Save a checkpoint every N steps
# convert_to_safetensors: true  # Convert the model to safetensors format when saving

# Optimizer configuration.
optimizer:
  type: adamw

  learning_rate: 1.0e-4
  weight_decay: 0.01

  beta1: 0.9
  beta2: 0.95

  update_clip: 3.0


# Learning rate scheduler configuration.
lr_scheduler:
  type: linear

  warmup_steps: 2000
  training_steps: 1000000
  

# Defaults to clip the L2 norm of gradients to 1.0.
# Set to null to disable gradient clipping by norm.
max_grad_norm: 1.0
# Defaults to not clip gradients by their absolute value.
# Set to a number to clip gradients by the specified max absolute value.
max_grad_value: null

init_threshold_step: 0
init_activated: false

gen_grad_wait: 0
gen_grad_warmup: 2000

lm_loss_threshold_upper: 1.05
lm_loss_threshold_lower: 0.95

kl_weight: 0.2

# uniformity_temp: 2.0
# uniformity_weight: 1.0
